/******************************************************************************
 * \file initialConditionGenerator.cpp
 * 
 * Main program for navier stokes FDM solver.
 * 
 * compile with
 * g++ openmp_main.cpp -fopenmp -o simulateNS
*/

#include <iostream>
#include <iomanip> // For std::setw
#include <fstream>
#include <sstream>
#include <string>
#include <map>
#include <vector>
#include <cmath>
#include <algorithm>
#include <random>
#include <omp.h>
#include <mpi.h>

// Directory where we will pull data from csv
const bool do_outputs = false;
const bool write_velocities = true;
const int outputFrequency = 150;

// Function to read array from the csv files generated by initialConditionGenerator.ipynb
// Function to read a CSV file into a 2D vector
std::vector<std::vector<double>> readCsv(const std::string& filename) {
    std::ifstream file(filename);
    std::string line;
    std::vector<std::vector<double>> matrix;
    bool isOneDimensional = true;

    // First pass: check if the file contains more than one line
    while (std::getline(file, line)) {
        if (!line.empty()) {  // Ensure the line is not empty
            std::vector<double> row;
            std::stringstream ss(line);
            std::string value;

            while (std::getline(ss, value, ',')) {
                row.push_back(std::stod(value));
            }

            matrix.push_back(row);

            // If we've read more than one line, it's not one-dimensional
            if (matrix.size() > 1) {
                isOneDimensional = false;
                break;
            }
        }
    }
    // If the file is one-dimensional, wrap the vector in another vector to maintain consistency
    if (isOneDimensional && !matrix.empty()) {
        std::vector<double> row = matrix[0];
        matrix.clear();         // Clear the existing content
        matrix.push_back(row);  // Push the 1D vector as a single row of the 2D vector
    }
    // If it's not one-dimensional, continue reading the rest of the file
    if (!isOneDimensional) {
        while (std::getline(file, line)) {
            if (!line.empty()) {  // Ensure the line is not empty
                std::vector<double> row;
                std::stringstream ss(line);
                std::string value;
                while (std::getline(ss, value, ',')) {
                    row.push_back(std::stod(value));
                }
                matrix.push_back(row);
            }
        }
    }
    return matrix;
}

// Function to read scalars from a file into a map
std::map<std::string, double> readScalars(const std::string& filename) {
    std::map<std::string, double> scalars;
    std::ifstream file(filename);
    std::string line;

    while (std::getline(file, line)) {
        size_t delimiterPos = line.find('=');
        std::string key = line.substr(0, delimiterPos);
        double value = std::stod(line.substr(delimiterPos + 1));
        scalars[key] = value;
    }

    return scalars;
}

// Function to write vectors to a csv file
// Function to write a 2D vector to a csv file
void writeToCsv(const std::vector<std::vector<double>>& data, const std::string& filePath) {
    std::ofstream file(filePath);
    if (!file.is_open()) {
        std::cerr << "Error opening file for writing: " << filePath << std::endl;
        return;
    }

    for (const auto& row : data) {
        for (size_t i = 0; i < row.size(); ++i) {
            file << row[i];
            if (i < row.size() - 1) {
                file << ",";
            }
        }
        file << "\n";
    }

    file.close();
}

// function for writing 1d vectors (ie the timer) to csv
void write1DToCsv(const std::vector<double>& data, const std::string& filePath) {
    std::ofstream file(filePath);
    if (!file.is_open()) {
        std::cerr << "Error opening file for writing: " << filePath << std::endl;
        return;
    }

    for (size_t i = 0; i < data.size(); ++i) {
        file << data[i];
        if (i < data.size() - 1) {
            file << ",";
        }
    }
    file << "\n";

    file.close();
}

// Function to return the index of the nearest meshpoint in the grid
int findClosestIndex(const std::vector<double>& arr, double value) {
    auto lower = std::lower_bound(arr.begin(), arr.end(), value);
    if (lower == arr.end()) return arr.size() - 1;  // If value is greater than any in array, return last index
    else if (lower == arr.begin()) return 0;  // If value is less than any in array, return first index
    else {
        auto prev = lower - 1;
        if ((value - *prev) <= (*lower - value)) return prev - arr.begin();  // Closer to the previous element
        else return lower - arr.begin();  // Closer to the current element
    }
}


int main(int argc, char** argv) {
    /**************************************************************************
     * STEP 1:
     * Read data from directory into arrays to use in program
     * This uses the 
    **************************************************************************/
    std::string directory_name;
    if (argc != 2) {
        std::cerr << "Usage: " << argv[0] << " <directory_name>" << std::endl;
        return 1;
    } else {
        directory_name = argv[1];
    }
    // Initializing MPI
    MPI_Init(&argc, &argv);
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    if(do_outputs) std::cout << std::setprecision(2);
    if(do_outputs) std::cout << std::scientific;
    if(do_outputs) std::cout << "Starting Simulation" << std::endl << std::endl << std::endl << std::endl;

    // Read 2D data from CSV files
    auto u0 = readCsv(directory_name + "/u.csv");
    auto v0 = readCsv(directory_name + "/v.csv");
    auto p0 = readCsv(directory_name + "/p.csv");
    auto gx0 = readCsv(directory_name + "/gx.csv");
    auto gy0 = readCsv(directory_name + "/gy.csv");
    auto Re0 = readCsv(directory_name + "/u.csv");       // Initialize Re array to save reynold's numbers

    // Read scalar data from a file
    auto scalars = readScalars(directory_name + "/scalars.dat");

    // Access scalar values
    double rho = scalars["rho"];        double mu = scalars["mu"];
    double xmin = scalars["xmin"];      double ymin = scalars["ymin"];
    double xmax = scalars["xmax"];      double ymax = scalars["ymax"];
    double dx = scalars["dx"];          double dy = scalars["dy"];
    double tmax = scalars["tmax"];      double dt = scalars["dt"];
    int Nx = static_cast<int>(scalars["Nx"]);
    int Ny = static_cast<int>(scalars["Ny"]);
    int NG = static_cast<int>(scalars["NG"]);
    // Geometric mean of active sim space for calculating Re
    double L = sqrt((xmax - xmin)*(ymax-ymin)); 

    // Initialize timing vector:
    std::vector<double> inner_loop_timer (static_cast<int>(tmax/dt + 0.5) + 1);
    /**************************************************************************
     * STEP 2:
     * Fields have been initialized on all processors, so now we define sets of
     * fields which are the decomposed domain. I will impose the restriction
     * that Ny must be evenly divisible by the number of mpi ranks
    **************************************************************************/
    if (Nx % size != 0) {
        std::cerr << "Number of x-cells " << Nx << " is not evenly divisible by MPI ranks " << size << std::endl;
        return 1;
    }
    int Nx_l = static_cast<int>((Nx+0.5)/size);
    std::vector<std::vector<double>>     u(Nx_l+2*NG, std::vector<double>(Ny+2*NG, 0.0));
    std::vector<std::vector<double>> u_new(Nx_l+2*NG, std::vector<double>(Ny+2*NG, 0.0));
    std::vector<std::vector<double>>     v(Nx_l+2*NG, std::vector<double>(Ny+2*NG, 0.0));
    std::vector<std::vector<double>> v_new(Nx_l+2*NG, std::vector<double>(Ny+2*NG, 0.0));
    std::vector<std::vector<double>>    Re(Nx_l+2*NG, std::vector<double>(Ny+2*NG, 0.0));
    std::vector<std::vector<double>>    gx(Nx_l+2*NG, std::vector<double>(Ny+2*NG, 0.0));
    std::vector<std::vector<double>>    gy(Nx_l+2*NG, std::vector<double>(Ny+2*NG, 0.0));
    std::vector<std::vector<double>>     p(Nx_l+2*NG, std::vector<double>(Ny+2*NG, 0.0));
    // populating the local arrays
    int i_global;   // i_global is the index in u0, i is the local index in u
    for (int j=0; j<Ny+2*NG; j++){
        for (int i=0; i<Nx_l+2*NG; i++){
            i_global = i + Nx_l*rank;
            u[i][j]  =  u0[i_global][j];
            v[i][j]  =  v0[i_global][j];
            gx[i][j] = gx0[i_global][j];
            gy[i][j] = gy0[i_global][j];
            p[i][j]  =  p0[i_global][j];
        }
    }

    /**************************************************************************
     * STEP 3:
     * Fields have been initialized, so now we get to start the iteration!
    **************************************************************************/        
    // Naively, I need to store different terms of the updating scheme in different variables
    double ut1, ut2, ut3, ut4, ut5;
    double vt1, vt2, vt3, vt4, vt5;
    double invRe;

    // time - to be iterated
    double t = 0; int ti = 0;
    // Useful sign variable for upwind scheme
    int sx, sy;

    // Calculating the first reynolds number iteration
    for (int j=NG; j<Ny+NG; j++){
        for (int i=NG; i<Nx_l+NG; i++){
            Re[i][j] = ((rho*sqrt(u[i][j]*u[i][j] + v[i][j]*v[i][j])*L) + 0.1)/mu;
        }
    }

    // Writing the initial conditions so they are all in the same spot
    if (write_velocities && rank==0) {
        writeToCsv(u0, "output_mpi/u_" + directory_name + "_" + std::to_string(ti) + ".csv");
        writeToCsv(v0, "output_mpi/v_" + directory_name + "_" + std::to_string(ti) + ".csv");
        writeToCsv(Re0, "output_mpi/Re_" + directory_name + "_" + std::to_string(ti) + ".csv");
    }

    while (t <= tmax)
    {   
        if (ti % outputFrequency == 0 && rank==0) std::cout << "Simulating timestep " << ti << " at time " << t << std::endl;

        // Starting timer for mpi
        double t1 = MPI_Wtime();

        // Performing "halo" exchange
        MPI_Request reqHalo[16];  // Increase the size as needed to handle all operations
        MPI_Status statuses[16];  // Corresponding status array
        int num_requests = 0;  // Counter for number of active requests

        int id_RSA = Nx_l;      // index of left-most Right-Side-Active cells
        int id_LSG = 0;         // index of left-most Left-Side-Ghost cells
        int id_LSA = NG;        // index of left-most Left-Side-Active cells
        int id_RSG = Nx_l+NG;   // index of left-most Right-Side-Ghost cells
        int umsg0 = 0; int umsg1 = 1; int umsg2 = 2; int umsg3 = 3; // naming message IDs for parsing
        int vmsg0 = 4; int vmsg1 = 5; int vmsg2 = 6; int vmsg3 = 7;
        // Defining the id of the "left" or "right" processors, with periodic boundaries
        int right_proc_id = (rank+1)%size;
        int left_proc_id = (rank-1)%size;
        if (left_proc_id < 0) left_proc_id = size-1;

        // Actual Halo Exchange
        // Treat all processors equivalently, and treat 0 and 1 as periodic, then we overwrite the periodic values with true 0-gradient
        // Sending RIGHT two active vectors to next rank, and corresponding receives
        if (rank < size-1) {
            MPI_Isend(u[id_RSA].data(), u[id_RSA].size(), MPI_DOUBLE, right_proc_id, umsg0, MPI_COMM_WORLD, &reqHalo[num_requests++]);
            MPI_Isend(u[id_RSA+1].data(), u[id_RSA+1].size(), MPI_DOUBLE, right_proc_id, umsg1, MPI_COMM_WORLD, &reqHalo[num_requests++]);
            MPI_Isend(v[id_RSA].data(), v[id_RSA].size(), MPI_DOUBLE, right_proc_id, vmsg0, MPI_COMM_WORLD, &reqHalo[num_requests++]);
            MPI_Isend(v[id_RSA+1].data(), v[id_RSA+1].size(), MPI_DOUBLE, right_proc_id, vmsg1, MPI_COMM_WORLD, &reqHalo[num_requests++]);

            MPI_Irecv(u[id_RSG].data(), u[id_RSG].size(), MPI_DOUBLE, right_proc_id, umsg2, MPI_COMM_WORLD, &reqHalo[num_requests++]);
            MPI_Irecv(u[id_RSG+1].data(), u[id_RSG+1].size(), MPI_DOUBLE, right_proc_id, umsg3, MPI_COMM_WORLD, &reqHalo[num_requests++]);
            MPI_Irecv(v[id_RSG].data(), v[id_RSG].size(), MPI_DOUBLE, right_proc_id, vmsg2, MPI_COMM_WORLD, &reqHalo[num_requests++]);
            MPI_Irecv(v[id_RSG+1].data(), v[id_RSG+1].size(), MPI_DOUBLE, right_proc_id, vmsg3, MPI_COMM_WORLD, &reqHalo[num_requests++]);
        }

        // Receiving from left_proc_id and sending to it
        if (rank > 0) {
            MPI_Irecv(u[id_LSG].data(), u[id_LSG].size(), MPI_DOUBLE, left_proc_id, umsg0, MPI_COMM_WORLD, &reqHalo[num_requests++]);
            MPI_Irecv(u[id_LSG+1].data(), u[id_LSG+1].size(), MPI_DOUBLE, left_proc_id, umsg1, MPI_COMM_WORLD, &reqHalo[num_requests++]);
            MPI_Irecv(v[id_LSG].data(), v[id_LSG].size(), MPI_DOUBLE, left_proc_id, vmsg0, MPI_COMM_WORLD, &reqHalo[num_requests++]);
            MPI_Irecv(v[id_LSG+1].data(), v[id_LSG+1].size(), MPI_DOUBLE, left_proc_id, vmsg1, MPI_COMM_WORLD, &reqHalo[num_requests++]);

            MPI_Isend(u[id_LSA].data(), u[id_LSA].size(), MPI_DOUBLE, left_proc_id, umsg2, MPI_COMM_WORLD, &reqHalo[num_requests++]);
            MPI_Isend(u[id_LSA+1].data(), u[id_LSA+1].size(), MPI_DOUBLE, left_proc_id, umsg3, MPI_COMM_WORLD, &reqHalo[num_requests++]);
            MPI_Isend(v[id_LSA].data(), v[id_LSA].size(), MPI_DOUBLE, left_proc_id, vmsg2, MPI_COMM_WORLD, &reqHalo[num_requests++]);
            MPI_Isend(v[id_LSA+1].data(), v[id_LSA+1].size(), MPI_DOUBLE, left_proc_id, vmsg3, MPI_COMM_WORLD, &reqHalo[num_requests++]);
        }

        // Wait for all exchanges to complete
        MPI_Waitall(num_requests, reqHalo, statuses);

        // Boundary conditions
        // Updating LHS of simulation - zero-gradient boundary conditions at rank0
        if (rank==0) {
            for (int i=0; i<NG; i++) {
                for (int j=NG; j<Ny+NG; j++){
                    u[i][j] = u[NG][j];
                    v[i][j] = v[NG][j];
                }
            }
        } 
        // Updating RHS of simulation - zero-gradient boundary condition at rank size
        if (rank == size-1) {
            for (int i=Nx_l+NG; i<Nx_l+2*NG; i++){
                for (int j=NG; j<Ny+NG; j++){
                    u[i][j] = u[Nx_l+NG-1][j];
                    v[i][j] = v[Nx_l+NG-1][j];
                }
            }
        }
        // Updating all tops and bottoms of divided domain
        for (int j=0; j<NG; j++){
            for(int i=0; i<Nx_l+2*NG; i++){
                u[i][j] = u[i][NG];
                v[i][j] = v[i][NG];
            }
        }
        for (int j=Ny+NG; j<Ny+2*NG; j++){
            for(int i=0; i<Nx_l+2*NG; i++){
                u[i][j] = u[i][Nx+NG-1];
                v[i][j] = v[i][Nx+NG-1];
            }
        }

        // Actual iteration
        for (int j=NG; j<Ny+NG; j++){
            for (int i=NG; i<Nx_l+NG; i++){
                // 1/Re - inverse reynolds number
                invRe = ((rho*sqrt(u[i][j]*u[i][j] + v[i][j]*v[i][j])*L) + 0.1)/mu;
                Re[i][j] = invRe;
                invRe = 1/invRe;
                // if (!isinf(invRe) || invRe > 5000.0) invRe = 5000.0;        // Necessary to ensure finite Re
                // Actual stencil - using upwind scheme
                
                // Defining an 's' to represent the sign of velocity
                sx = u[i][j] >= 0 ? 1 : -1;
                sy = v[i][j] >= 0 ? 1 : -1;
                
                // dpdx
                ut1 = sx/(2*dx)*(3*p[i][j] - 4*p[i-1*sx][j] + p[i-2*sx][j]);
                // d2udx2
                ut2 = 1/(dx*dx) * (u[i][j] - 2*u[i-1*sx][j] + u[i-2*sx][j]);
                // d2udy2
                ut3 = 1/(dy*dy) * (u[i][j] - 2*u[i][j-1*sy] + u[i][j-2*sy]);
                // d(u2)dx
                ut4 = sx/(2*dx)*(3*u[i][j]*u[i][j] - 4*u[i-1*sx][j]*u[i-1*sx][j] + u[i-2*sx][j]*u[i-2*sx][j]);
                // d(uv)dy
                ut5 = sy/(2*dy)*(3*u[i][j]*v[i][j] - 4*u[i][j-1*sy]*v[i][j-1*sy] + u[i][j-2*sy]*v[i][j-2*sy]);

                // dpdy
                vt1 = sy/(2*dy)*(3*p[i][j] - 4*p[i][j-1*sy] + p[i][j-2*sy]);
                // d2vdx2
                vt2 = 1/(dx*dx) * (v[i][j] - 2*v[i-1*sx][j] + v[i-2*sx][j]);
                // d2vdy2
                vt3 = 1/(dy*dy) * (v[i][j] - 2*v[i][j-1*sy] + v[i][j-2*sy]);
                // d(uv)dx
                vt4 = sx/(2*dx)*(3*u[i][j]*v[i][j] - 4*u[i-1*sx][j]*v[i-1*sx][j] + u[i-2*sx][j]*v[i-2*sx][j]);
                // d(v2)dy
                vt5 = sy/(2*dy)*(3*v[i][j]*v[i][j] - 4*v[i][j-1*sy]*v[i][j-1*sy] + v[i][j-2*sy]*v[i][j-2*sy]);

                // Updating u and v
                v_new[i][j] = v[i][j] + dt*(-vt1 + invRe*(vt2 + vt3) - vt4 - vt5 + gy[i][j]);
                u_new[i][j] = u[i][j] + dt*(-ut1 + invRe*(ut2 + ut3) - ut4 - ut5 + gx[i][j]);
            }
        }

        // Updating the next time step's u and v before writing to file
        for(int j=0; j<Ny+2*NG; j++) {
            for(int i=0; i<Nx_l+2*NG; i++) {
                u[i][j] = u_new[i][j];
                v[i][j] = v_new[i][j];
            }
        }

        // Stopping time
        if (rank == 0) {
            double t2 = MPI_Wtime();
            inner_loop_timer[ti] = t2-t1;
        } 

        // Update time iterator
        t += dt; ti++;

        // Output to file
        if (ti % outputFrequency == 0 && write_velocities) {
            std::vector<MPI_Request> reqs;
            std::vector<MPI_Status> statuses;

            // Prepare arrays on the root for full data
            std::vector<std::vector<double>> u_out(Nx + 2 * NG, std::vector<double>(Ny + 2 * NG, 0.0));
            std::vector<std::vector<double>> v_out = u_out;
            std::vector<std::vector<double>> Re_out = u_out;

            // Sending all NON-ROOT arrays to the root array
            if (rank != 0) {
                for (int i = NG; i < Nx_l + NG; i++) {  // Send only the active region, exclude ghost cells
                    int send_id_u = rank * 100000 + (i - NG);
                    int send_id_v = rank * 100000 + (i - NG) + Nx_l * 100;  // Ensure unique tags for u and v
                    reqs.push_back(MPI_Request());
                    MPI_Isend(u[i].data(), u[i].size(), MPI_DOUBLE, 0, send_id_u, MPI_COMM_WORLD, &reqs.back());
                    reqs.push_back(MPI_Request());
                    MPI_Isend(v[i].data(), v[i].size(), MPI_DOUBLE, 0, send_id_v, MPI_COMM_WORLD, &reqs.back());
                }
            }

            // Receiving all NON-ROOT arrays to root array
            if (rank == 0) {
                for (int src = 1; src < size; src++) {
                    int start_idx = NG + src * Nx_l;
                    for (int i = 0; i < Nx_l; i++) {
                        int idx = start_idx + i;
                        int recv_id_u = src * 100000 + i;
                        int recv_id_v = src * 100000 + i + Nx_l * 100;  // Match the unique tags for u and v
                        reqs.push_back(MPI_Request());
                        MPI_Irecv(u_out[idx].data(), u_out[idx].size(), MPI_DOUBLE, src, recv_id_u, MPI_COMM_WORLD, &reqs.back());
                        reqs.push_back(MPI_Request());
                        MPI_Irecv(v_out[idx].data(), v_out[idx].size(), MPI_DOUBLE, src, recv_id_v, MPI_COMM_WORLD, &reqs.back());
                    }
                }
                statuses.resize(reqs.size());
                MPI_Waitall(reqs.size(), reqs.data(), statuses.data());

                
                // Filling u_out and v_out with the root's data
                for (int i = 0; i < Nx_l+NG; i++) {
                    u_out[i] = u[i];
                    v_out[i] = v[i];
                }
            }

            // Calculating the Reynold's number
            if (rank == 0) {
                for (int j = NG; j < Ny + NG; j++) {
                    for (int i = NG; i < Nx + NG; i++) {
                        Re_out[i][j] = ((rho * sqrt(u_out[i][j] * u_out[i][j] + v_out[i][j] * v_out[i][j]) * L) + 0.1) / mu;
                    }
                }
                writeToCsv(u_out, "output_mpi/u_" + directory_name + "_" + std::to_string(ti) + ".csv");
                writeToCsv(v_out, "output_mpi/v_" + directory_name + "_" + std::to_string(ti) + ".csv");
                writeToCsv(Re_out, "output_mpi/Re_" + directory_name + "_" + std::to_string(ti) + ".csv");
            }
        }



    }   // End of time iteration
    // Write final time clock list to an output
    if (rank == 0) {
        write1DToCsv(inner_loop_timer, "output_mpi/timer_" + directory_name + "_" + std::to_string(size) + ".csv");
    }
    MPI_Finalize();
    return 0;
}